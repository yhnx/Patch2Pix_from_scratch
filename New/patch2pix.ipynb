{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"afaf191b","cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T08:25:39.142638Z","iopub.execute_input":"2025-10-15T08:25:39.143286Z","iopub.status.idle":"2025-10-15T08:25:41.023869Z","shell.execute_reply.started":"2025-10-15T08:25:39.143262Z","shell.execute_reply":"2025-10-15T08:25:41.023270Z"}},"outputs":[],"execution_count":1},{"id":"7d96e94e","cell_type":"code","source":"#  Loading the Pretrained NCNet from Google Drive\n\n!pip install gdown\nimport gdown\n\n# File ID from your link\nfile_id = \"10GZ0x3CmObKzbAg1GKQhrkPeSLRpD4Rp\"\nurl = f\"https://drive.google.com/uc?id={file_id}\"\n\n# Save location\noutput = \"ncnet_checkpoint.pth\"\ngdown.download(url, output, quiet=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T08:25:41.024837Z","iopub.execute_input":"2025-10-15T08:25:41.025147Z","iopub.status.idle":"2025-10-15T08:25:47.762760Z","shell.execute_reply.started":"2025-10-15T08:25:41.025129Z","shell.execute_reply":"2025-10-15T08:25:47.761862Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.19.1)\nRequirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.5)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.7)\nRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.15.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.8.3)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n","output_type":"stream"},{"name":"stderr","text":"Downloading...\nFrom: https://drive.google.com/uc?id=10GZ0x3CmObKzbAg1GKQhrkPeSLRpD4Rp\nTo: /kaggle/working/ncnet_checkpoint.pth\n100%|██████████| 6.00k/6.00k [00:00<00:00, 9.97MB/s]\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'ncnet_checkpoint.pth'"},"metadata":{}}],"execution_count":2},{"id":"7e9d9571-0898-4b63-8b43-b7323f51aace","cell_type":"code","source":"import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Module, Conv2d\nfrom torch.autograd import Variable\nfrom torch.nn.parameter import Parameter\nfrom torch.nn.modules.conv import _ConvNd\nfrom torch.nn.modules.utils import _quadruple\nimport torchvision.models as models\nfrom collections import OrderedDict\nimport torch.utils.model_zoo as model_zoo","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T09:23:21.521028Z","iopub.execute_input":"2025-10-15T09:23:21.521713Z","iopub.status.idle":"2025-10-15T09:23:21.526093Z","shell.execute_reply.started":"2025-10-15T09:23:21.521688Z","shell.execute_reply":"2025-10-15T09:23:21.525317Z"}},"outputs":[],"execution_count":33},{"id":"fc2e50b0-9e95-4ed3-8792-9f17d309326f","cell_type":"code","source":"\"\"\"\nThis script is an adapted version of \nhttps://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py \nThe goal is to keep ResNet* as only feature extractor, \nso the code can be used independent of the types of specific tasks,\ni.e., classification or regression. \n\"\"\"\n\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n    \nclass ResNet(nn.Module):\n    PRETRAINED_URLs = {\n        'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n        'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n        'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n        'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n        'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n    }\n    \n    def __init__(self):\n        super().__init__()\n        \n    def _build_model(self, block, layers):\n        self.inplanes = 64\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n        return nn.Sequential(*layers)\n\n    def forward(self, x, early_feat=False):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        if early_feat:\n            return x\n        x = self.layer4(x)\n        return x\n    \n    def forward_all(self, x, feat_list=[], early_feat=True):\n        feat_list.append(x)\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        feat_list.append(x)\n        \n        x = self.maxpool(x)\n        x = self.layer1(x)\n        feat_list.append(x)\n        \n        x = self.layer2(x)\n        feat_list.append(x)\n        \n        x = self.layer3(x)\n        feat_list.append(x)\n        \n        if not early_feat:\n            x = self.layer4(x)\n            feat_list.append(x)\n    \n    def load_pretrained_(self, ignore='fc'):\n        print('Initialize ResNet using pretrained model from {}'.format(self.pretrained_url))\n        state_dict = model_zoo.load_url(self.pretrained_url)\n        new_state_dict = OrderedDict()\n        for k, v in state_dict.items():\n            if ignore in k:\n                continue\n            new_state_dict[k] = v\n        self.load_state_dict(new_state_dict)\n\n    def change_stride(self, target='layer3'):\n        layer = getattr(self, target)\n        layer[0].conv1.stride = (1, 1)\n        layer[0].conv2.stride = (1, 1)\n        layer[0].downsample[0].stride = (1, 1) \n\nclass ResNet34(ResNet):\n    def __init__(self):\n        super().__init__()\n        self.pretrained_url = self.PRETRAINED_URLs['resnet34']\n        self._build_model(BasicBlock, [3, 4, 6, 3])\n\nclass ResNet50(ResNet):\n    def __init__(self):\n        super().__init__()\n        self.pretrained_url = self.PRETRAINED_URLs['resnet50']\n        self._build_model(Bottleneck, [3, 4, 6, 3])\n        \nclass ResNet101(ResNet):\n    def __init__(self):\n        super().__init__()\n        self.pretrained_url = self.PRETRAINED_URLs['resnet101']\n        self._build_model(Bottleneck, [3, 4, 23, 3])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T09:23:27.346084Z","iopub.execute_input":"2025-10-15T09:23:27.346370Z","iopub.status.idle":"2025-10-15T09:23:27.365822Z","shell.execute_reply.started":"2025-10-15T09:23:27.346349Z","shell.execute_reply":"2025-10-15T09:23:27.365094Z"}},"outputs":[],"execution_count":34},{"id":"4d573a5c-0620-416b-b789-9d62d7f4a061","cell_type":"code","source":"def Softmax1D(x,dim):\n    x_k = torch.max(x,dim)[0].unsqueeze(dim)\n    x -= x_k.expand_as(x)\n    exp_x = torch.exp(x)\n    return torch.div(exp_x,torch.sum(exp_x,dim).unsqueeze(dim).expand_as(x))\n\ndef featureL2Norm(feature):\n    epsilon = 1e-6\n    norm = torch.pow(torch.sum(torch.pow(feature,2),1)+epsilon,0.5).unsqueeze(1).expand_as(feature)\n    feat_norm = torch.div(feature,norm)\n    return feat_norm\n\nclass FeatureExtraction(torch.nn.Module):\n    def __init__(self, train_fe=False, feature_extraction_cnn='resnet101', feature_extraction_model_file='', normalization=True, last_layer='', use_cuda=True):\n        super(FeatureExtraction, self).__init__()\n        self.normalization = normalization\n        self.feature_extraction_cnn=feature_extraction_cnn\n        if feature_extraction_cnn == 'vgg':\n            self.model = models.vgg16(pretrained=True)\n            # keep feature extraction network up to indicated layer\n            vgg_feature_layers=['conv1_1','relu1_1','conv1_2','relu1_2','pool1','conv2_1',\n                         'relu2_1','conv2_2','relu2_2','pool2','conv3_1','relu3_1',\n                         'conv3_2','relu3_2','conv3_3','relu3_3','pool3','conv4_1',\n                         'relu4_1','conv4_2','relu4_2','conv4_3','relu4_3','pool4',\n                         'conv5_1','relu5_1','conv5_2','relu5_2','conv5_3','relu5_3','pool5']\n            if last_layer=='':\n                last_layer = 'pool4'\n            last_layer_idx = vgg_feature_layers.index(last_layer)\n            self.model = nn.Sequential(*list(self.model.features.children())[:last_layer_idx+1])\n        # for resnet below\n        resnet_feature_layers = ['conv1','bn1','relu','maxpool','layer1','layer2','layer3','layer4']\n        if feature_extraction_cnn=='resnet101':\n            self.model = models.resnet101(pretrained=True)            \n            if last_layer=='':\n                last_layer = 'layer3'                            \n            resnet_module_list = [getattr(self.model,l) for l in resnet_feature_layers]\n            last_layer_idx = resnet_feature_layers.index(last_layer)\n            self.model = nn.Sequential(*resnet_module_list[:last_layer_idx+1])\n\n        if feature_extraction_cnn=='resnet101fpn':\n            if feature_extraction_model_file!='':\n                resnet = models.resnet101(pretrained=True) \n                # swap stride (2,2) and (1,1) in first layers (PyTorch ResNet is slightly different to caffe2 ResNet)\n                # this is required for compatibility with caffe2 models\n                resnet.layer2[0].conv1.stride=(2,2)\n                resnet.layer2[0].conv2.stride=(1,1)\n                resnet.layer3[0].conv1.stride=(2,2)\n                resnet.layer3[0].conv2.stride=(1,1)\n                resnet.layer4[0].conv1.stride=(2,2)\n                resnet.layer4[0].conv2.stride=(1,1)\n            else:\n                resnet = models.resnet101(pretrained=True) \n            resnet_module_list = [getattr(resnet,l) for l in resnet_feature_layers]\n            conv_body = nn.Sequential(*resnet_module_list)\n            self.model = fpn_body(conv_body,\n                                  resnet_feature_layers,\n                                  fpn_layers=['layer1','layer2','layer3'],\n                                  normalize=normalization,\n                                  hypercols=True)\n            if feature_extraction_model_file!='':\n                self.model.load_pretrained_weights(feature_extraction_model_file)\n\n        if feature_extraction_cnn == 'densenet201':\n            self.model = models.densenet201(pretrained=True)\n            # keep feature extraction network up to denseblock3\n            # self.model = nn.Sequential(*list(self.model.features.children())[:-3])\n            # keep feature extraction network up to transitionlayer2\n            self.model = nn.Sequential(*list(self.model.features.children())[:-4])\n        if train_fe==False:\n            # freeze parameters\n            for param in self.model.parameters():\n                param.requires_grad = False\n        # move to GPU\n        if use_cuda:\n            self.model = self.model.cuda()\n        \n    def forward(self, image_batch):\n        features = self.model(image_batch)\n        if self.normalization and not self.feature_extraction_cnn=='resnet101fpn':\n            features = featureL2Norm(features)\n        return features\n    \nclass FeatureCorrelation(torch.nn.Module):\n    def __init__(self,shape='3D',normalization=True):\n        super(FeatureCorrelation, self).__init__()\n        self.normalization = normalization\n        self.shape=shape\n        self.ReLU = nn.ReLU()\n    \n    def forward(self, feature_A, feature_B):        \n        if self.shape=='3D':\n            b,c,h,w = feature_A.size()\n            # reshape features for matrix multiplication\n            feature_A = feature_A.transpose(2,3).contiguous().view(b,c,h*w)\n            feature_B = feature_B.view(b,c,h*w).transpose(1,2)\n            # perform matrix mult.\n            feature_mul = torch.bmm(feature_B,feature_A)\n            # indexed [batch,idx_A=row_A+h*col_A,row_B,col_B]\n            correlation_tensor = feature_mul.view(b,h,w,h*w).transpose(2,3).transpose(1,2)\n        elif self.shape=='4D':\n            b,c,hA,wA = feature_A.size()\n            b,c,hB,wB = feature_B.size()\n            # reshape features for matrix multiplication\n            feature_A = feature_A.view(b,c,hA*wA).transpose(1,2) # size [b,c,h*w]\n            feature_B = feature_B.view(b,c,hB*wB) # size [b,c,h*w]\n            # perform matrix mult.\n            feature_mul = torch.bmm(feature_A,feature_B)\n            # indexed [batch,row_A,col_A,row_B,col_B]\n            correlation_tensor = feature_mul.view(b,hA,wA,hB,wB).unsqueeze(1)\n        \n        if self.normalization:\n            correlation_tensor = featureL2Norm(self.ReLU(correlation_tensor))\n            \n        return correlation_tensor\n\nclass NeighConsensus(torch.nn.Module):\n    def __init__(self, use_cuda=True, kernel_sizes=[3,3,3], channels=[10,10,1], symmetric_mode=True):\n        super(NeighConsensus, self).__init__()\n        self.symmetric_mode = symmetric_mode\n        self.kernel_sizes = kernel_sizes\n        self.channels = channels\n        num_layers = len(kernel_sizes)\n        nn_modules = list()\n        for i in range(num_layers):\n            if i==0:\n                ch_in = 1\n            else:\n                ch_in = channels[i-1]\n            ch_out = channels[i]\n            k_size = kernel_sizes[i]\n            nn_modules.append(Conv4d(in_channels=ch_in,out_channels=ch_out,kernel_size=k_size,bias=True))\n            nn_modules.append(nn.ReLU(inplace=True))\n        self.conv = nn.Sequential(*nn_modules)        \n        if use_cuda:\n            self.conv.cuda()\n\n    def forward(self, x):\n        if self.symmetric_mode:\n            # apply network on the input and its \"transpose\" (swapping A-B to B-A ordering of the correlation tensor),\n            # this second result is \"transposed back\" to the A-B ordering to match the first result and be able to add together\n            x = self.conv(x)+self.conv(x.permute(0,1,4,5,2,3)).permute(0,1,4,5,2,3)\n            # because of the ReLU layers in between linear layers, \n            # this operation is different than convolving a single time with the filters+filters^T\n            # and therefore it makes sense to do this.\n        else:\n            x = self.conv(x)\n        return x\n\ndef MutualMatching(corr4d):\n    # mutual matching\n    batch_size,ch,fs1,fs2,fs3,fs4 = corr4d.size()\n\n    corr4d_B=corr4d.view(batch_size,fs1*fs2,fs3,fs4) # [batch_idx,k_A,i_B,j_B]\n    corr4d_A=corr4d.view(batch_size,fs1,fs2,fs3*fs4)\n\n    # get max\n    corr4d_B_max,_=torch.max(corr4d_B,dim=1,keepdim=True)\n    corr4d_A_max,_=torch.max(corr4d_A,dim=3,keepdim=True)\n\n    eps = 1e-5\n    corr4d_B=corr4d_B/(corr4d_B_max+eps)\n    corr4d_A=corr4d_A/(corr4d_A_max+eps)\n\n    corr4d_B=corr4d_B.view(batch_size,1,fs1,fs2,fs3,fs4)\n    corr4d_A=corr4d_A.view(batch_size,1,fs1,fs2,fs3,fs4)\n\n    corr4d=corr4d*(corr4d_A*corr4d_B) # parenthesis are important for symmetric output \n    return corr4d\n\ndef MutualNorm(corr4d):\n    # mutual matching\n    batch_size,ch,fs1,fs2,fs3,fs4 = corr4d.size()\n\n    corr4d_B=corr4d.view(batch_size,fs1*fs2,fs3,fs4) # [batch_idx,k_A,i_B,j_B]\n    corr4d_A=corr4d.view(batch_size,fs1,fs2,fs3*fs4)\n\n    # get max\n    corr4d_B_max,_=torch.max(corr4d_B,dim=1,keepdim=True)\n    corr4d_A_max,_=torch.max(corr4d_A,dim=3,keepdim=True)\n\n    eps = 1e-5\n    corr4d_B=corr4d_B/(corr4d_B_max+eps)\n    corr4d_A=corr4d_A/(corr4d_A_max+eps)\n\n    corr4d_B=corr4d_B.view(batch_size,1,fs1,fs2,fs3,fs4)\n    corr4d_A=corr4d_A.view(batch_size,1,fs1,fs2,fs3,fs4)\n    return (corr4d_A*corr4d_B)\n\ndef maxpool4d(corr4d_hres,k_size=4):\n    slices=[]\n    for i in range(k_size):\n        for j in range(k_size):\n            for k in range(k_size):\n                for l in range(k_size):\n                    sl = corr4d_hres[:,0,i::k_size,j::k_size,k::k_size,l::k_size].unsqueeze(0)\n                    slices.append(sl)\n\n    slices=torch.cat(tuple(slices),dim=1)\n    corr4d,max_idx=torch.max(slices,dim=1,keepdim=True)\n    max_l=torch.fmod(max_idx,k_size)\n    max_k=torch.fmod(max_idx.sub(max_l).div(k_size),k_size)\n    max_j=torch.fmod(max_idx.sub(max_l).div(k_size).sub(max_k).div(k_size),k_size)\n    max_i=max_idx.sub(max_l).div(k_size).sub(max_k).div(k_size).sub(max_j).div(k_size)\n    # i,j,k,l represent the *relative* coords of the max point in the box of size k_size*k_size*k_size*k_size\n    return (corr4d,max_i,max_j,max_k,max_l)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T09:05:16.870779Z","iopub.execute_input":"2025-10-15T09:05:16.871387Z","iopub.status.idle":"2025-10-15T09:05:16.895159Z","shell.execute_reply.started":"2025-10-15T09:05:16.871361Z","shell.execute_reply":"2025-10-15T09:05:16.894368Z"}},"outputs":[],"execution_count":24},{"id":"58b1b1b4-f395-4edf-ab4f-fab26304054f","cell_type":"code","source":"def corr_to_matches(corr4d, delta4d=None, ksize=1, do_softmax=True, scale='positive', \n                    invert_matching_direction=False, return_indices=True):\n    to_cuda = lambda x: x.to(corr4d.device) if corr4d.is_cuda else x        \n    batch_size,ch,fs1,fs2,fs3,fs4 = corr4d.size()  # b, c, h, w, h, w\n    if scale=='centered':\n        XA,YA=np.meshgrid(np.linspace(-1,1,fs2*ksize),np.linspace(-1,1,fs1*ksize))\n        XB,YB=np.meshgrid(np.linspace(-1,1,fs4*ksize),np.linspace(-1,1,fs3*ksize))\n    elif scale=='positive':\n        # Upsampled resolution linear space\n        XA,YA=np.meshgrid(np.linspace(0,1,fs2*ksize),np.linspace(0,1,fs1*ksize))\n        XB,YB=np.meshgrid(np.linspace(0,1,fs4*ksize),np.linspace(0,1,fs3*ksize))\n    # Index meshgrid for current resolution\n    JA,IA=np.meshgrid(range(fs2),range(fs1)) \n    JB,IB=np.meshgrid(range(fs4),range(fs3))\n    \n    XA,YA=Variable(to_cuda(torch.FloatTensor(XA))),Variable(to_cuda(torch.FloatTensor(YA)))\n    XB,YB=Variable(to_cuda(torch.FloatTensor(XB))),Variable(to_cuda(torch.FloatTensor(YB)))\n\n    JA,IA=Variable(to_cuda(torch.LongTensor(JA).view(1,-1))),Variable(to_cuda(torch.LongTensor(IA).view(1,-1)))\n    JB,IB=Variable(to_cuda(torch.LongTensor(JB).view(1,-1))),Variable(to_cuda(torch.LongTensor(IB).view(1,-1)))\n    \n    if invert_matching_direction:\n        nc_A_Bvec=corr4d.view(batch_size,fs1,fs2,fs3*fs4)\n\n        if do_softmax:\n            nc_A_Bvec=torch.nn.functional.softmax(nc_A_Bvec,dim=3)\n\n        # Max and argmax\n        match_A_vals,idx_A_Bvec=torch.max(nc_A_Bvec,dim=3)\n        score=match_A_vals.view(batch_size,-1)\n        \n        # Pick the indices for the best score\n        iB=IB.view(-1)[idx_A_Bvec.view(-1)].view(batch_size,-1).contiguous()  # b, h1*w1\n        jB=JB.view(-1)[idx_A_Bvec.view(-1)].view(batch_size,-1).contiguous()\n        iA=IA.expand_as(iB).contiguous()\n        jA=JA.expand_as(jB).contiguous()\n        \n    else:    \n        nc_B_Avec=corr4d.view(batch_size,fs1*fs2,fs3,fs4) # [batch_idx,k_A,i_B,j_B]\n        if do_softmax:\n            nc_B_Avec=torch.nn.functional.softmax(nc_B_Avec,dim=1)\n\n        match_B_vals,idx_B_Avec=torch.max(nc_B_Avec,dim=1)\n        score=match_B_vals.view(batch_size,-1)\n        \n        iA=IA.view(-1)[idx_B_Avec.view(-1)].view(batch_size,-1).contiguous() # b, h2*w2\n        jA=JA.view(-1)[idx_B_Avec.view(-1)].view(batch_size,-1).contiguous() \n        iB=IB.expand_as(iA).contiguous()\n        jB=JB.expand_as(jA).contiguous()\n    \n    if delta4d is not None: # relocalization, it is also the case ksize > 1\n        # The shift within the pooling window reference to (0,0,0,0)\n        delta_iA, delta_jA, delta_iB, delta_jB = delta4d  # b, 1, h1, w1, h2, w2 \n        \n        \"\"\" Original implementation\n        # Reorder the indices according \n        diA = delta_iA.squeeze(0).squeeze(0)[iA.view(-1), jA.view(-1), iB.view(-1), jB.view(-1)] \n        djA = delta_jA.squeeze(0).squeeze(0)[iA.view(-1), jA.view(-1), iB.view(-1), jB.view(-1)]        \n        diB = delta_iB.squeeze(0).squeeze(0)[iA.view(-1), jA.view(-1), iB.view(-1), jB.view(-1)]\n        djB = delta_jB.squeeze(0).squeeze(0)[iA.view(-1), jA.view(-1), iB.view(-1), jB.view(-1)]\n\n        # *ksize place the pixel to the 1st location in upsampled 4D-Volumn\n        iA = iA * ksize + diA.expand_as(iA)\n        jA = jA * ksize + djA.expand_as(jA)\n        iB = iB * ksize + diB.expand_as(iB)\n        jB = jB * ksize + djB.expand_as(jB)\n        \"\"\"\n        \n        # Support batches\n        for ibx in range(batch_size):\n            diA = delta_iA[ibx][0][iA[ibx], jA[ibx], iB[ibx], jB[ibx]]  # h*w\n            djA = delta_jA[ibx][0][iA[ibx], jA[ibx], iB[ibx], jB[ibx]]\n            diB = delta_iB[ibx][0][iA[ibx], jA[ibx], iB[ibx], jB[ibx]]\n            djB = delta_jB[ibx][0][iA[ibx], jA[ibx], iB[ibx], jB[ibx]]\n            \n            iA[ibx] = iA[ibx] * ksize + diA\n            jA[ibx] = jA[ibx] * ksize + djA\n            iB[ibx] = iB[ibx] * ksize + diB\n            jB[ibx] = jB[ibx] * ksize + djB\n\n    xA = XA[iA.view(-1), jA.view(-1)].view(batch_size, -1)\n    yA = YA[iA.view(-1), jA.view(-1)].view(batch_size, -1)\n    xB = XB[iB.view(-1), jB.view(-1)].view(batch_size, -1)\n    yB = YB[iB.view(-1), jB.view(-1)].view(batch_size, -1)\n        \n    if return_indices:\n        return (jA,iA,jB,iB,score)\n    else:\n        return (xA,yA,xB,yB,score)    \n    \ndef corr_to_matches_topk(corr4d, delta4d=None, topk=1, ksize=1, do_softmax=True,                     \n                         invert_matching_direction=False):\n\n    device = corr4d.device\n    batch_size, ch, fs1, fs2, fs3, fs4 = corr4d.size()  # b, c, h, w, h, w\n\n    # Index meshgrid for current resolution\n    JA, IA = np.meshgrid(range(fs2), range(fs1)) \n    JB, IB = np.meshgrid(range(fs4), range(fs3))    \n    JA, IA = torch.LongTensor(JA).view(1,-1).to(device), torch.LongTensor(IA).view(1,-1).to(device)\n    JB, IB = torch.LongTensor(JB).view(1,-1).to(device), torch.LongTensor(IB).view(1,-1).to(device)\n\n    if invert_matching_direction:\n        nc_A_Bvec = corr4d.view(batch_size, fs1, fs2, fs3 * fs4)\n\n        if do_softmax:\n            nc_A_Bvec = torch.nn.functional.softmax(nc_A_Bvec, dim=3)\n\n        # Max and argmax\n        match_A_vals, idx_A_Bvec = torch.topk(nc_A_Bvec, topk, dim=3, largest=True, sorted=True)    \n        score = match_A_vals.view(batch_size, -1)\n\n        # Pick the indices for the best score\n        iB = IB.view(-1)[idx_A_Bvec.view(-1)].view(batch_size, -1, topk).contiguous()\n        jB = JB.view(-1)[idx_A_Bvec.view(-1)].view(batch_size, -1, topk).contiguous()\n        iA = IA.unsqueeze(-1).expand_as(iB).contiguous()\n        jA = JA.unsqueeze(-1).expand_as(jB).contiguous()\n\n    else:    \n        nc_B_Avec = corr4d.view(batch_size, fs1 * fs2, fs3, fs4) # [batch_idx,k_A,i_B,j_B]\n        if do_softmax:\n            nc_B_Avec = torch.nn.functional.softmax(nc_B_Avec, dim=1)\n\n        match_B_vals, idx_B_Avec = torch.topk(nc_B_Avec, topk, dim=1, largest=True, sorted=True)\n        score = match_B_vals.view(batch_size, -1)\n\n        iA = IA.view(-1)[idx_B_Avec.view(-1)].view(batch_size, topk, -1).contiguous()\n        jA = JA.view(-1)[idx_B_Avec.view(-1)].view(batch_size, topk, -1).contiguous() \n        iB = IB.unsqueeze(1).expand_as(iA).contiguous() \n        jB = JB.unsqueeze(1).expand_as(jA).contiguous()\n        \n    iA = iA.view(batch_size, -1)\n    jA = jA.view(batch_size, -1)\n    iB = iB.view(batch_size, -1)\n    jB = jB.view(batch_size, -1)   \n\n    if delta4d is not None: # relocalization, it is also the case ksize > 1\n        # The shift within the pooling window reference to (0,0,0,0)\n        delta_iA, delta_jA, delta_iB, delta_jB = delta4d\n\n        # Support batches\n        for ibx in range(batch_size):\n            diA = delta_iA[ibx][0][iA[ibx], jA[ibx], iB[ibx], jB[ibx]]  # h*w\n            djA = delta_jA[ibx][0][iA[ibx], jA[ibx], iB[ibx], jB[ibx]]\n            diB = delta_iB[ibx][0][iA[ibx], jA[ibx], iB[ibx], jB[ibx]]\n            djB = delta_jB[ibx][0][iA[ibx], jA[ibx], iB[ibx], jB[ibx]]\n            \n            iA[ibx] = iA[ibx] * ksize + diA\n            jA[ibx] = jA[ibx] * ksize + djA\n            iB[ibx] = iB[ibx] * ksize + diB\n            jB[ibx] = jB[ibx] * ksize + djB\n\n    return (jA, iA, jB, iB, score)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T09:05:22.101414Z","iopub.execute_input":"2025-10-15T09:05:22.101708Z","iopub.status.idle":"2025-10-15T09:05:22.123803Z","shell.execute_reply.started":"2025-10-15T09:05:22.101686Z","shell.execute_reply":"2025-10-15T09:05:22.123127Z"}},"outputs":[],"execution_count":25},{"id":"cbbf1084-d580-4776-a957-237db73e8959","cell_type":"code","source":"def conv4d(data, filters, bias=None, permute_filters=True, use_half=False):\n    b, c, h, w, d, t = data.size()\n\n    data = data.permute(\n        2, 0, 1, 3, 4, 5\n    ).contiguous()  # permute to avoid making contiguous inside loop\n\n    # Same permutation is done with filters, unless already provided with permutation\n    if permute_filters:\n        filters = filters.permute(\n            2, 0, 1, 3, 4, 5\n        ).contiguous()  # permute to avoid making contiguous inside loop\n\n    c_out = filters.size(1)\n    if use_half:\n        output = Variable(\n            torch.HalfTensor(h, b, c_out, w, d, t), requires_grad=data.requires_grad\n        )\n    else:\n        output = Variable(\n            torch.zeros(h, b, c_out, w, d, t), requires_grad=data.requires_grad\n        )\n\n    padding = filters.size(0) // 2\n    if use_half:\n        Z = Variable(torch.zeros(padding, b, c, w, d, t).half())\n    else:\n        Z = Variable(torch.zeros(padding, b, c, w, d, t))\n\n    if data.is_cuda:\n        Z = Z.cuda(data.get_device())\n        output = output.cuda(data.get_device())\n\n    data_padded = torch.cat((Z, data, Z), 0)\n\n    for i in range(output.size(0)):  # loop on first feature dimension\n        # convolve with center channel of filter (at position=padding)\n        output[i, :, :, :, :, :] = F.conv3d(\n            data_padded[i + padding, :, :, :, :, :],\n            filters[padding, :, :, :, :, :],\n            bias=bias,\n            stride=1,\n            padding=padding,\n        )\n        # convolve with upper/lower channels of filter (at postions [:padding] [padding+1:])\n        for p in range(1, padding + 1):\n            output[i, :, :, :, :, :] = output[i, :, :, :, :, :] + F.conv3d(\n                data_padded[i + padding - p, :, :, :, :, :],\n                filters[padding - p, :, :, :, :, :],\n                bias=None,\n                stride=1,\n                padding=padding,\n            )\n            output[i, :, :, :, :, :] = output[i, :, :, :, :, :] + F.conv3d(\n                data_padded[i + padding + p, :, :, :, :, :],\n                filters[padding + p, :, :, :, :, :],\n                bias=None,\n                stride=1,\n                padding=padding,\n            )\n\n    output = output.permute(1, 2, 0, 3, 4, 5).contiguous()\n    return output\n\n\nclass Conv4d(_ConvNd):\n    \"\"\"Applies a 4D convolution over an input signal composed of several input\n    planes.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        bias=True,\n        pre_permuted_filters=True,\n    ):\n        # stride, dilation and groups !=1 functionality not tested\n        stride = 1\n        dilation = 1\n        groups = 1\n        # zero padding is added automatically in conv4d function to preserve tensor size\n        padding = 0\n        kernel_size = _quadruple(kernel_size)\n        stride = _quadruple(stride)\n        padding = _quadruple(padding)\n        dilation = _quadruple(dilation)\n\n        super().__init__(\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride,\n            padding,\n            dilation,\n            False,\n            _quadruple(0),\n            groups,\n            bias,\n            padding_mode=\"zeros\",\n        )\n\n        # weights will be sliced along one dimension during convolution loop\n        # make the looping dimension to be the first one in the tensor,\n        # so that we don't need to call contiguous() inside the loop\n        self.pre_permuted_filters = pre_permuted_filters\n        if self.pre_permuted_filters:\n            self.weight.data = self.weight.data.permute(2, 0, 1, 3, 4, 5).contiguous()\n        self.use_half = False\n\n    def forward(self, input):\n        return conv4d(\n            input,\n            self.weight,\n            bias=self.bias,\n            permute_filters=not self.pre_permuted_filters,\n            use_half=self.use_half,\n        )  # filters pre-permuted in constructor\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T09:05:25.182659Z","iopub.execute_input":"2025-10-15T09:05:25.182929Z","iopub.status.idle":"2025-10-15T09:05:25.194604Z","shell.execute_reply.started":"2025-10-15T09:05:25.182911Z","shell.execute_reply":"2025-10-15T09:05:25.194104Z"}},"outputs":[],"execution_count":26},{"id":"cea55e2c-f924-4c22-be22-282d2907df34","cell_type":"code","source":"# -----------------------------\n# ImMatchNet (fixed)\n# -----------------------------\nclass ImMatchNet(nn.Module):\n    def __init__(self, \n                 feature_extraction_cnn='resnet101', \n                 feature_extraction_last_layer='',\n                 feature_extraction_model_file=None,\n                 return_correlation=False,  \n                 ncons_kernel_sizes=[3,3,3],\n                 ncons_channels=[10,10,1],\n                 normalize_features=True,\n                 train_fe=False,\n                 use_cuda=True,\n                 relocalization_k_size=0,\n                 half_precision=False,\n                 checkpoint=None):\n        \n        super(ImMatchNet, self).__init__()\n        # Load checkpoint\n        if checkpoint is not None and checkpoint != '':\n            print('Loading checkpoint...')\n            checkpoint = torch.load(checkpoint, map_location=lambda storage, loc: storage)\n            checkpoint['state_dict'] = OrderedDict([(k.replace('vgg', 'model'), v) for k, v in checkpoint['state_dict'].items()])\n            # override relevant parameters\n            print('Using checkpoint parameters:')\n            ncons_channels = checkpoint['args'].ncons_channels\n            print('  ncons_channels: '+str(ncons_channels))\n            ncons_kernel_sizes = checkpoint['args'].ncons_kernel_sizes\n            print('  ncons_kernel_sizes: '+str(ncons_kernel_sizes))            \n\n        self.use_cuda = use_cuda\n        self.normalize_features = normalize_features\n        self.return_correlation = return_correlation\n        self.relocalization_k_size = relocalization_k_size\n        self.half_precision = half_precision\n        \n        self.FeatureExtraction = FeatureExtraction(train_fe=train_fe,\n                                                   feature_extraction_cnn=feature_extraction_cnn,\n                                                   feature_extraction_model_file=feature_extraction_model_file,\n                                                   last_layer=feature_extraction_last_layer,\n                                                   normalization=normalize_features,\n                                                   use_cuda=self.use_cuda)\n        \n        self.FeatureCorrelation = FeatureCorrelation(shape='4D', normalization=False)\n\n        self.NeighConsensus = NeighConsensus(use_cuda=self.use_cuda,\n                                             kernel_sizes=ncons_kernel_sizes,\n                                             channels=ncons_channels)\n\n        # Load weights\n        if checkpoint is not None and checkpoint != '':\n            print('Copying weights...')\n            for name, param in self.FeatureExtraction.state_dict().items():\n                if 'num_batches_tracked' not in name:\n                    self.FeatureExtraction.state_dict()[name].copy_(checkpoint['state_dict']['FeatureExtraction.' + name])    \n            for name, param in self.NeighConsensus.state_dict().items():\n                self.NeighConsensus.state_dict()[name].copy_(checkpoint['state_dict']['NeighConsensus.' + name])\n            print('Done!')\n        \n        self.FeatureExtraction.eval()\n\n        if self.half_precision:\n            for p in self.NeighConsensus.parameters():\n                p.data = p.data.half()\n            for l in self.NeighConsensus.conv:\n                if isinstance(l, Conv4d):\n                    l.use_half = True\n                    \n    def forward(self, tnf_batch): \n        feature_A = self.FeatureExtraction(tnf_batch['source_image'])\n        feature_B = self.FeatureExtraction(tnf_batch['target_image'])\n        if self.half_precision:\n            feature_A = feature_A.half()\n            feature_B = feature_B.half()\n            \n        corr4d = self.FeatureCorrelation(feature_A, feature_B)\n\n        if self.relocalization_k_size > 1:\n            corr4d, max_i, max_j, max_k, max_l = maxpool4d(corr4d, k_size=self.relocalization_k_size)\n\n        corr4d = MutualMatching(corr4d)\n        corr4d = self.NeighConsensus(corr4d)\n        corr4d = MutualMatching(corr4d)\n        \n        if self.relocalization_k_size > 1:\n            delta4d = (max_i, max_j, max_k, max_l)\n            return (corr4d, delta4d)\n        else:\n            return corr4d\n \n    def forward_feat(self, featA, featB, normalize=True): \n        if normalize:\n            feature_A = featureL2Norm(featA)\n            feature_B = featureL2Norm(featB)\n        else:\n            feature_A = featA\n            feature_B = featB\n        if self.half_precision:\n            feature_A = feature_A.half()\n            feature_B = feature_B.half()\n\n        corr4d = self.FeatureCorrelation(feature_A, feature_B)\n        if self.relocalization_k_size > 1:\n            corr4d, max_i, max_j, max_k, max_l = maxpool4d(corr4d, k_size=self.relocalization_k_size)\n        corr4d = MutualMatching(corr4d)\n        corr4d = self.NeighConsensus(corr4d)\n        corr4d = MutualMatching(corr4d)\n        if self.relocalization_k_size > 1:\n            delta4d = (max_i, max_j, max_k, max_l)\n            return (corr4d, delta4d)\n        else:\n            return corr4d","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T09:05:31.606953Z","iopub.execute_input":"2025-10-15T09:05:31.607693Z","iopub.status.idle":"2025-10-15T09:05:31.619899Z","shell.execute_reply.started":"2025-10-15T09:05:31.607668Z","shell.execute_reply":"2025-10-15T09:05:31.619038Z"}},"outputs":[],"execution_count":27}]}