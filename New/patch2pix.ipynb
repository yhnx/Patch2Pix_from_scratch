{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-16T17:33:23.595258Z","iopub.execute_input":"2025-10-16T17:33:23.595550Z","iopub.status.idle":"2025-10-16T17:33:23.601884Z","shell.execute_reply.started":"2025-10-16T17:33:23.595530Z","shell.execute_reply":"2025-10-16T17:33:23.600787Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"!pip install gdown\n\n# Patch2Pix pretrained\n!gdown 1ZbJIE0LcZ3Oti-h8zU72JRL7LryVxJOh -O /kaggle/working/patch2pix_pretrained.pth\n\n# NCNet IVD pretrained (optional)\n!gdown 10GZ0x3CmObKzbAg1GKQhrkPeSLRpD4Rp -O /kaggle/working/ncn_ivd_5ep.pth\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T17:37:30.811268Z","iopub.execute_input":"2025-10-16T17:37:30.811600Z","iopub.status.idle":"2025-10-16T17:37:43.373157Z","shell.execute_reply.started":"2025-10-16T17:37:30.811576Z","shell.execute_reply":"2025-10-16T17:37:43.372120Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.19.1)\nRequirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.5)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.7)\nRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.15.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.8.3)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1ZbJIE0LcZ3Oti-h8zU72JRL7LryVxJOh\nFrom (redirected): https://drive.google.com/uc?id=1ZbJIE0LcZ3Oti-h8zU72JRL7LryVxJOh&confirm=t&uuid=6e0ad579-3e0f-406c-a992-16a53edab4f6\nTo: /kaggle/working/patch2pix_pretrained.pth\n100%|████████████████████████████████████████| 126M/126M [00:01<00:00, 65.9MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=10GZ0x3CmObKzbAg1GKQhrkPeSLRpD4Rp\nTo: /kaggle/working/ncn_ivd_5ep.pth\n100%|██████████████████████████████████████| 6.00k/6.00k [00:00<00:00, 19.4MB/s]\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"import torch\nfrom argparse import Namespace\n\n# allow Namespace objects inside the pickle\ntorch.serialization.add_safe_globals([Namespace])\n\nckpt = torch.load(\"/kaggle/working/patch2pix_pretrained.pth\",\n                  map_location=device, weights_only=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T17:40:48.141602Z","iopub.execute_input":"2025-10-16T17:40:48.141994Z","iopub.status.idle":"2025-10-16T17:40:48.250509Z","shell.execute_reply.started":"2025-10-16T17:40:48.141968Z","shell.execute_reply":"2025-10-16T17:40:48.249416Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"print(type(ckpt))\nif isinstance(ckpt, dict):\n    print(list(ckpt.keys())[:10])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T17:41:03.419080Z","iopub.execute_input":"2025-10-16T17:41:03.419542Z","iopub.status.idle":"2025-10-16T17:41:03.426903Z","shell.execute_reply.started":"2025-10-16T17:41:03.419505Z","shell.execute_reply":"2025-10-16T17:41:03.425487Z"}},"outputs":[{"name":"stdout","text":"<class 'dict'>\n['backbone', 'feat_idx', 'change_stride', 'ctopk', 'regressor_config', 'state_dict']\n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"\"\"\"\nThis script is an adapted version of \nhttps://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py \nThe goal is to keep ResNet* as only feature extractor, \nso the code can be used independent of the types of specific tasks,\ni.e., classification or regression. \n\"\"\"\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\nfrom collections import OrderedDict\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n    \nclass ResNet(nn.Module):\n    PRETRAINED_URLs = {\n        'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n        'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n        'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n        'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n        'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n    }\n    \n    def __init__(self):\n        super().__init__()\n        \n    def _build_model(self, block, layers):\n        self.inplanes = 64\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n        return nn.Sequential(*layers)\n\n    def forward(self, x, early_feat=False):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        if early_feat:\n            return x\n        x = self.layer4(x)\n        return x\n    \n    def forward_all(self, x, feat_list=[], early_feat=True):\n        feat_list.append(x)\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        feat_list.append(x)\n        \n        x = self.maxpool(x)\n        x = self.layer1(x)\n        feat_list.append(x)\n        \n        x = self.layer2(x)\n        feat_list.append(x)\n        \n        x = self.layer3(x)\n        feat_list.append(x)\n        \n        if not early_feat:\n            x = self.layer4(x)\n            feat_list.append(x)\n    \n    def load_pretrained_(self, ignore='fc'):\n        print('Initialize ResNet using pretrained model from {}'.format(self.pretrained_url))\n        state_dict = model_zoo.load_url(self.pretrained_url)\n        new_state_dict = OrderedDict()\n        for k, v in state_dict.items():\n            if ignore in k:\n                continue\n            new_state_dict[k] = v\n        self.load_state_dict(new_state_dict)\n\n    def change_stride(self, target='layer3'):\n        layer = getattr(self, target)\n        layer[0].conv1.stride = (1, 1)\n        layer[0].conv2.stride = (1, 1)\n        layer[0].downsample[0].stride = (1, 1) \n\nclass ResNet34(ResNet):\n    def __init__(self):\n        super().__init__()\n        self.pretrained_url = self.PRETRAINED_URLs['resnet34']\n        self._build_model(BasicBlock, [3, 4, 6, 3])\n\nclass ResNet50(ResNet):\n    def __init__(self):\n        super().__init__()\n        self.pretrained_url = self.PRETRAINED_URLs['resnet50']\n        self._build_model(Bottleneck, [3, 4, 6, 3])\n        \nclass ResNet101(ResNet):\n    def __init__(self):\n        super().__init__()\n        self.pretrained_url = self.PRETRAINED_URLs['resnet101']\n        self._build_model(Bottleneck, [3, 4, 23, 3])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T17:33:23.603289Z","iopub.execute_input":"2025-10-16T17:33:23.603662Z","iopub.status.idle":"2025-10-16T17:33:23.630396Z","shell.execute_reply.started":"2025-10-16T17:33:23.603629Z","shell.execute_reply":"2025-10-16T17:33:23.629578Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.parameter import Parameter\n\nL2Normalize = lambda feat, dim: feat / torch.pow(torch.sum(torch.pow(feat, 2), dim=dim) + 1e-6, 0.5).unsqueeze(dim)\n\ndef cal_conv_out_size(w, kernel_size, stride, padding):\n    return (w - kernel_size + 2 * padding) // stride + 1\n\ndef maxpool4d(corr4d_hres, k_size=4):\n    slices=[]\n    for i in range(k_size):\n        for j in range(k_size):\n            for k in range(k_size):\n                for l in range(k_size):\n                    sl = corr4d_hres[:,:,i::k_size,j::k_size,k::k_size,l::k_size] # Support batches\n                    slices.append(sl)\n                    \n    slices = torch.cat(tuple(slices),dim=1)  # B, ksize*4, h1, w1, h2, w2\n    corr4d, max_idx = torch.max(slices,dim=1,keepdim=True)\n    \n    # i,j,k,l represent the *relative* coords of the max point in the box of size k_size*k_size*k_size*k_size    \n    if torch.__version__ >= '1.6.0':\n        max_l=torch.fmod(max_idx,k_size)\n        max_k=torch.fmod(max_idx.sub(max_l).floor_divide(k_size),k_size)\n        max_j=torch.fmod(max_idx.sub(max_l).floor_divide(k_size).sub(max_k).floor_divide(k_size),k_size)\n        max_i=max_idx.sub(max_l).floor_divide(k_size).sub(max_k).floor_divide(k_size).sub(max_j).floor_divide(k_size)\n    else:\n        max_l=torch.fmod(max_idx,k_size)\n        max_k=torch.fmod(max_idx.sub(max_l).div(k_size),k_size)\n        max_j=torch.fmod(max_idx.sub(max_l).div(k_size).sub(max_k).div(k_size),k_size)\n        max_i=max_idx.sub(max_l).div(k_size).sub(max_k).div(k_size).sub(max_j).div(k_size)\n    return (corr4d,max_i,max_j,max_k,max_l)\n\nclass FeatCorrelation(torch.nn.Module):\n    def __init__(self, shape='4D'):\n        super().__init__()\n        self.shape = shape\n    \n    def forward(self, feat1, feat2):        \n        b, c, h1, w1 = feat1.size()\n        b, c, h2, w2 = feat2.size()\n        feat1 = feat1.view(b, c, h1*w1).transpose(1, 2) # size [b, h1*w1, c]\n        feat2 = feat2.view(b, c, h2*w2)  # size [b, c, h2*w2]\n        \n        # Matrix multiplication\n        correlation = torch.bmm(feat1, feat2)  # [b, h1*w1, h2*w2]\n        if self.shape == '3D':\n            correlation = correlation.view(b, h1, w1, h2*w2).permute(0, 3, 1, 2)  # [b, h2*w2, h1, w1]            \n        elif self.shape == '4D':\n            correlation = correlation.view(b, h1, w1, h2, w2).unsqueeze(1) # [b, 1, h1, w1, h2, w2]\n        return correlation\n\n    \nclass FeatRegressNet(nn.Module):\n    def __init__(self, config, psize=16, out_dim=5):\n        super().__init__()\n        self.psize = psize\n        self.conv_strs = config.conv_strs if 'conv_strs' in config else [2] * len(config.conv_kers)\n        self.conv_dims = config.conv_dims\n        self.conv_kers = config.conv_kers\n        self.feat_comb = config.feat_comb  # Combine 2 feature maps before the conv or after the conv\n        self.feat_dim = config.feat_dim if self.feat_comb == 'post' else 2 * config.feat_dim\n        self.fc_in_dim = config.conv_dims[-1] * 2 if self.feat_comb == 'post' else config.conv_dims[-1]\n        \n        # Build layers    \n        self.conv = self.make_conv_layers(self.feat_dim, self.conv_dims, self.conv_kers)\n        self.fc = self.make_fc_layers(self.fc_in_dim, config.fc_dims, out_dim)        \n        print(f'FeatRegressNet:  feat_comb:{self.feat_comb} ' \\\n              f'psize:{self.psize} out:{out_dim} ' \\\n              f'feat_dim:{self.feat_dim} conv_kers:{self.conv_kers} ' \\\n              f'conv_dims:{self.conv_dims} conv_str:{self.conv_strs} ' \n              )        \n        \n    def make_conv_layers(self, in_dim, conv_dims, conv_kers, bias=False):\n        layers = []\n        w = self.psize  # Initial spatial size        \n        for out_dim, kernel_size, stride in zip(conv_dims, conv_kers, self.conv_strs):\n            layers.append(nn.Conv2d(in_dim, out_dim, kernel_size, stride=stride, padding=1, bias=bias))\n            layers.append(nn.BatchNorm2d(out_dim))\n            w = cal_conv_out_size(w, kernel_size, stride, 1)\n            in_dim = out_dim\n        layers.append(nn.ReLU())        \n        # To make sure spatial dim goes to 1, one can also use AdaptiveMaxPool\n        layers.append(nn.MaxPool2d(kernel_size=w))\n        return nn.Sequential(*layers)\n        \n    def make_fc_layers(self, in_dim, fc_dims, fc_out_dim):\n        layers = []\n        for out_dim in fc_dims:\n            layers.append(nn.Linear(in_dim, out_dim))\n            layers.append(nn.BatchNorm1d(out_dim)),\n            layers.append(nn.ReLU())            \n            in_dim = out_dim\n            \n        # Final layer\n        layers.append(nn.Linear(in_dim, fc_out_dim))\n        return nn.Sequential(*layers)\n    \n    def forward(self, feat1, feat2):\n        # feat1, feat2: shape (N, D, 16, 16)       \n        if self.feat_comb == 'pre':\n            feat = torch.cat([feat1, feat2], dim=1)            \n            feat = self.conv(feat)  # N, D, 1, 1\n        else:\n            feat1 = self.conv(feat1)\n            feat2 = self.conv(feat2)            \n            feat = torch.cat([feat1, feat2], dim=1)  # N, D, 1, 1\n        feat = feat.view(-1, feat.shape[1])\n        out = self.fc(feat)  # N, 5\n        return out \n    \ndef init_optimizer(params, config):\n    if config.opt == 'adam':\n        optimizer = torch.optim.Adam(params, lr=config.lr_init, weight_decay=config.weight_decay)\n        print('Setup  Adam optimizer(lr={},wd={})'.format(config.lr_init, config.weight_decay))\n\n    elif config.opt == 'sgd':\n        optimizer = torch.optim.SGD(params, momentum=0.9, lr=config.lr_init, weight_decay=config.weight_decay)\n        print('Setup  SGD optimizer(lr={},wd={},mom=0.9)'.format(config.lr_init, config.weight_decay))\n\n    if config.optimizer_dict:\n        optimizer.load_state_dict(config.optimizer_dict)\n\n    # Schedule learning rate decay  lr_decay = ['name', params] or None\n    lr_scheduler = None\n    if 'lr_decay' in config and config.lr_decay:\n        if config.lr_decay[0] == 'step':\n            decay_factor, decay_step = float(config.lr_decay[1]), int(config.lr_decay[2])\n            last_epoch = config.start_epoch - 1\n            lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, \n                                                           step_size=decay_step, \n                                                           gamma=decay_factor, \n                                                           last_epoch=last_epoch)\n            print(f'Setup StepLR Decay: decay_factor={decay_factor} '\n                  f'step={decay_step} last_epoch={last_epoch}')\n\n        elif config.lr_decay[0] == 'multistep':\n            decay_factor = float(config.lr_decay[1])\n            decay_steps = [int(v) for v in config.lr_decay[2::]]\n            last_epoch = config.start_epoch - 1\n            lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n                                                                milestones=decay_steps,\n                                                                gamma=decay_factor, \n                                                                last_epoch=last_epoch)\n            print(f'Setup MultiStepLR Decay: decay_factor={decay_factor} '\n                  f'steps={decay_steps} last_epoch={last_epoch}')\n\n        if config.lr_scheduler_dict and lr_scheduler:\n            lr_scheduler.load_state_dict(config.lr_scheduler_dict)\n    return optimizer, lr_scheduler     \n    \ndef xavier_init_func_(m):\n    classname = m.__class__.__name__\n    if classname.startswith('Conv'):\n        nn.init.xavier_uniform_(m.weight.data)\n        if m.bias is not None:  # Incase bias is turned off            \n            nn.init.constant_(m.bias.data, 0.0)\n    elif classname.find('Linear') != -1:\n        nn.init.xavier_uniform_(m.weight.data)\n        if m.bias is not None:  # Incase bias is turned off            \n            nn.init.constant_(m.bias.data, 0.0)\n    elif classname.find('BatchNorm2d') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0.0)\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T17:33:23.862470Z","iopub.execute_input":"2025-10-16T17:33:23.862848Z","iopub.status.idle":"2025-10-16T17:33:23.890791Z","shell.execute_reply.started":"2025-10-16T17:33:23.862820Z","shell.execute_reply":"2025-10-16T17:33:23.889825Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"def select_local_patch_feats(feats1, feats2, ibatch, imatches, \n                             feat_idx=[1, 2, 3, 4],\n                             feats_downsample=[1, 2, 2, 2, 2],\n                             psize=16, ptype='center'):\n    dy, dx = torch.meshgrid(torch.arange(psize), torch.arange(psize))\n    dx = dx.flatten().view(1, -1).to(imatches.device)\n    dy = dy.flatten().view(1, -1).to(imatches.device)\n\n    if ptype == 'center':\n        shift = psize // 2\n        dy -= shift\n        dx -= shift    \n    \n    _, _, h1, w1 = feats1[0].shape\n    _, _, h2, w2 = feats2[0].shape\n    x1, y1, x2, y2 = imatches.permute(1, 0).long()\n\n    # Ids for local patch\n    get_x_pids = lambda x, w, ds: ((x.view(-1, 1) + dx).view(-1) // ds).long().clamp(min=0, max=w//ds-1)\n    get_y_pids = lambda y, h, ds: ((y.view(-1, 1) + dy).view(-1) // ds).long().clamp(min=0, max=h//ds-1)\n\n    # Collect features for local matches\n    f1s, f2s = [], []\n    for j, (fmap1, fmap2) in enumerate(zip(feats1, feats2)):\n        if j not in feat_idx:\n            continue \n        ds = np.prod(feats_downsample[0:j+1])\n        f1s.append(fmap1[ibatch, :, get_y_pids(y1, h1, ds), get_x_pids(x1, w1, ds)])\n        f2s.append(fmap2[ibatch, :, get_y_pids(y2, h2, ds), get_x_pids(x2, w2, ds)])\n        \n    f1s = torch.cat(f1s, dim=0) # D, N*16\n    f2s = torch.cat(f2s, dim=0) # D, N*16\n    return f1s, f2s, dx.squeeze(), dy.squeeze()\n\ndef filter_coarse(coarse_matches, match_scores, ncn_thres=0.0, mutual=True, ptmax=None):\n    matches = []\n    scores = []\n    for imatches, iscores in  zip(coarse_matches, match_scores):\n        _, ids, counts = np.unique(imatches.cpu().data.numpy(), axis=0, return_index=True, return_counts=True)\n        if mutual:\n            # Consider only if they are multual consistant \n            ids = ids[counts > 1]\n            #print(len(imatches), len(ids))\n            \n        if len(ids) > 0:\n            iscores = iscores[ids]\n            imatches = imatches[ids]\n\n        # NC score filtering\n        ids = torch.nonzero(iscores.flatten() > ncn_thres, as_tuple=False).flatten()\n        \n        # Cut or fill upto ptmax for memory control\n        if ptmax: \n            if len(ids) == 0:\n                # insert a random match\n                ids = torch.tensor([0, 0, 0, 0]).long()\n            iids = np.arange(len(ids))\n            np.random.shuffle(iids)\n            iids = np.tile(iids, (ptmax // len(ids) + 1))[:ptmax]\n            ids = ids[iids]\n            \n        if len(ids) > 0: \n            iscores = iscores[ids]\n            imatches = imatches[ids]\n            \n        matches.append(imatches)\n        scores.append(iscores)\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T17:33:23.892361Z","iopub.execute_input":"2025-10-16T17:33:23.892674Z","iopub.status.idle":"2025-10-16T17:33:23.914536Z","shell.execute_reply.started":"2025-10-16T17:33:23.892650Z","shell.execute_reply":"2025-10-16T17:33:23.913505Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"class NeighConsensus(torch.nn.Module):\n    def __init__(self, use_cuda=True, kernel_sizes=[3,3,3], channels=[10,10,1], symmetric_mode=True):\n        super(NeighConsensus, self).__init__()\n        self.symmetric_mode = symmetric_mode\n        self.kernel_sizes = kernel_sizes\n        self.channels = channels\n        num_layers = len(kernel_sizes)\n        nn_modules = list()\n        for i in range(num_layers):\n            if i==0:\n                ch_in = 1\n            else:\n                ch_in = channels[i-1]\n            ch_out = channels[i]\n            k_size = kernel_sizes[i]\n            nn_modules.append(Conv4d(in_channels=ch_in,out_channels=ch_out,kernel_size=k_size,bias=True))\n            nn_modules.append(nn.ReLU(inplace=True))\n        self.conv = nn.Sequential(*nn_modules)        \n        if use_cuda and torch.cuda.is_available():\n            self.conv.cuda()\n\n\n    def forward(self, x):\n        if self.symmetric_mode:\n            # apply network on the input and its \"transpose\" (swapping A-B to B-A ordering of the correlation tensor),\n            # this second result is \"transposed back\" to the A-B ordering to match the first result and be able to add together\n            x = self.conv(x)+self.conv(x.permute(0,1,4,5,2,3)).permute(0,1,4,5,2,3)\n            # because of the ReLU layers in between linear layers, \n            # this operation is different than convolving a single time with the filters+filters^T\n            # and therefore it makes sense to do this.\n        else:\n            x = self.conv(x)\n        return x\n\ndef MutualMatching(corr4d):\n    # mutual matching\n    batch_size,ch,fs1,fs2,fs3,fs4 = corr4d.size()\n\n    corr4d_B=corr4d.view(batch_size,fs1*fs2,fs3,fs4) # [batch_idx,k_A,i_B,j_B]\n    corr4d_A=corr4d.view(batch_size,fs1,fs2,fs3*fs4)\n\n    # get max\n    corr4d_B_max,_=torch.max(corr4d_B,dim=1,keepdim=True)\n    corr4d_A_max,_=torch.max(corr4d_A,dim=3,keepdim=True)\n\n    eps = 1e-5\n    corr4d_B=corr4d_B/(corr4d_B_max+eps)\n    corr4d_A=corr4d_A/(corr4d_A_max+eps)\n\n    corr4d_B=corr4d_B.view(batch_size,1,fs1,fs2,fs3,fs4)\n    corr4d_A=corr4d_A.view(batch_size,1,fs1,fs2,fs3,fs4)\n\n    corr4d=corr4d*(corr4d_A*corr4d_B) # parenthesis are important for symmetric output \n    return corr4d\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T17:33:23.915610Z","iopub.execute_input":"2025-10-16T17:33:23.915969Z","iopub.status.idle":"2025-10-16T17:33:23.941296Z","shell.execute_reply.started":"2025-10-16T17:33:23.915946Z","shell.execute_reply":"2025-10-16T17:33:23.940280Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"\ndef corr_to_matches(corr4d, delta4d=None, ksize=1, do_softmax=True, scale='positive', \n                    invert_matching_direction=False, return_indices=True):\n    to_cuda = lambda x: x.to(corr4d.device) if corr4d.is_cuda else x        \n    batch_size,ch,fs1,fs2,fs3,fs4 = corr4d.size()  # b, c, h, w, h, w\n    if scale=='centered':\n        XA,YA=np.meshgrid(np.linspace(-1,1,fs2*ksize),np.linspace(-1,1,fs1*ksize))\n        XB,YB=np.meshgrid(np.linspace(-1,1,fs4*ksize),np.linspace(-1,1,fs3*ksize))\n    elif scale=='positive':\n        # Upsampled resolution linear space\n        XA,YA=np.meshgrid(np.linspace(0,1,fs2*ksize),np.linspace(0,1,fs1*ksize))\n        XB,YB=np.meshgrid(np.linspace(0,1,fs4*ksize),np.linspace(0,1,fs3*ksize))\n    # Index meshgrid for current resolution\n    JA,IA=np.meshgrid(range(fs2),range(fs1)) \n    JB,IB=np.meshgrid(range(fs4),range(fs3))\n    \n    XA,YA=Variable(to_cuda(torch.FloatTensor(XA))),Variable(to_cuda(torch.FloatTensor(YA)))\n    XB,YB=Variable(to_cuda(torch.FloatTensor(XB))),Variable(to_cuda(torch.FloatTensor(YB)))\n\n    JA,IA=Variable(to_cuda(torch.LongTensor(JA).view(1,-1))),Variable(to_cuda(torch.LongTensor(IA).view(1,-1)))\n    JB,IB=Variable(to_cuda(torch.LongTensor(JB).view(1,-1))),Variable(to_cuda(torch.LongTensor(IB).view(1,-1)))\n    \n    if invert_matching_direction:\n        nc_A_Bvec=corr4d.view(batch_size,fs1,fs2,fs3*fs4)\n\n        if do_softmax:\n            nc_A_Bvec=torch.nn.functional.softmax(nc_A_Bvec,dim=3)\n\n        # Max and argmax\n        match_A_vals,idx_A_Bvec=torch.max(nc_A_Bvec,dim=3)\n        score=match_A_vals.view(batch_size,-1)\n        \n        # Pick the indices for the best score\n        iB=IB.view(-1)[idx_A_Bvec.view(-1)].view(batch_size,-1).contiguous()  # b, h1*w1\n        jB=JB.view(-1)[idx_A_Bvec.view(-1)].view(batch_size,-1).contiguous()\n        iA=IA.expand_as(iB).contiguous()\n        jA=JA.expand_as(jB).contiguous()\n        \n    else:    \n        nc_B_Avec=corr4d.view(batch_size,fs1*fs2,fs3,fs4) # [batch_idx,k_A,i_B,j_B]\n        if do_softmax:\n            nc_B_Avec=torch.nn.functional.softmax(nc_B_Avec,dim=1)\n\n        match_B_vals,idx_B_Avec=torch.max(nc_B_Avec,dim=1)\n        score=match_B_vals.view(batch_size,-1)\n        \n        iA=IA.view(-1)[idx_B_Avec.view(-1)].view(batch_size,-1).contiguous() # b, h2*w2\n        jA=JA.view(-1)[idx_B_Avec.view(-1)].view(batch_size,-1).contiguous() \n        iB=IB.expand_as(iA).contiguous()\n        jB=JB.expand_as(jA).contiguous()\n    \n    if delta4d is not None: # relocalization, it is also the case ksize > 1\n        # The shift within the pooling window reference to (0,0,0,0)\n        delta_iA, delta_jA, delta_iB, delta_jB = delta4d  # b, 1, h1, w1, h2, w2 \n        \n        \"\"\" Original implementation\n        # Reorder the indices according \n        diA = delta_iA.squeeze(0).squeeze(0)[iA.view(-1), jA.view(-1), iB.view(-1), jB.view(-1)] \n        djA = delta_jA.squeeze(0).squeeze(0)[iA.view(-1), jA.view(-1), iB.view(-1), jB.view(-1)]        \n        diB = delta_iB.squeeze(0).squeeze(0)[iA.view(-1), jA.view(-1), iB.view(-1), jB.view(-1)]\n        djB = delta_jB.squeeze(0).squeeze(0)[iA.view(-1), jA.view(-1), iB.view(-1), jB.view(-1)]\n\n        # *ksize place the pixel to the 1st location in upsampled 4D-Volumn\n        iA = iA * ksize + diA.expand_as(iA)\n        jA = jA * ksize + djA.expand_as(jA)\n        iB = iB * ksize + diB.expand_as(iB)\n        jB = jB * ksize + djB.expand_as(jB)\n        \"\"\"\n        \n        # Support batches\n        for ibx in range(batch_size):\n            diA = delta_iA[ibx][0][iA[ibx], jA[ibx], iB[ibx], jB[ibx]]  # h*w\n            djA = delta_jA[ibx][0][iA[ibx], jA[ibx], iB[ibx], jB[ibx]]\n            diB = delta_iB[ibx][0][iA[ibx], jA[ibx], iB[ibx], jB[ibx]]\n            djB = delta_jB[ibx][0][iA[ibx], jA[ibx], iB[ibx], jB[ibx]]\n            \n            iA[ibx] = iA[ibx] * ksize + diA\n            jA[ibx] = jA[ibx] * ksize + djA\n            iB[ibx] = iB[ibx] * ksize + diB\n            jB[ibx] = jB[ibx] * ksize + djB\n\n    xA = XA[iA.view(-1), jA.view(-1)].view(batch_size, -1)\n    yA = YA[iA.view(-1), jA.view(-1)].view(batch_size, -1)\n    xB = XB[iB.view(-1), jB.view(-1)].view(batch_size, -1)\n    yB = YB[iB.view(-1), jB.view(-1)].view(batch_size, -1)\n        \n    if return_indices:\n        return (jA,iA,jB,iB,score)\n    else:\n        return (xA,yA,xB,yB,score)   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T17:33:23.942322Z","iopub.execute_input":"2025-10-16T17:33:23.942587Z","iopub.status.idle":"2025-10-16T17:33:23.965910Z","shell.execute_reply.started":"2025-10-16T17:33:23.942559Z","shell.execute_reply":"2025-10-16T17:33:23.964869Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\n\nclass Patch2Pix(nn.Module):    \n    def __init__(self, config):\n        super().__init__()\n        self.device = config.device\n        self.backbone = config.backbone\n        self.change_stride = config.change_stride        \n        self.upsample = 16\n        self.feats_downsample = [1, 2, 2, 2, 2]\n        feat_dims = [3, 64, 64, 128, 256]  # Resnet34 block feature out dims \n        \n        # Initialize necessary network components\n        self.extract = globals()[self.backbone]()\n        if self.change_stride:\n            self.extract.change_stride(target='layer3')\n            self.upsample //= 2            \n            self.feats_downsample[-1] = 1        \n        print(f'Initialize Patch2Pix: backbone={self.backbone} '\n              f'cstride={self.change_stride} upsample={self.upsample}')\n        \n        self.combine = FeatCorrelation(shape='4D')\n        self.ncn = NeighConsensus(kernel_sizes=[3, 3], channels=[16, 1])\n        \n        # Initialize regressor\n        self.regressor_config = config.regressor_config\n        if not self.regressor_config:\n            # If no regressor defined, model only computes coarse matches\n            self.regress_mid = None\n            self.regress_fine = None\n        else:\n            print(f'Init regressor {self.regressor_config}')\n            self.regr_batch = config.regr_batch\n            self.feat_idx = config.feat_idx            \n            feat_dim = 0  # Regressor's input feature dim\n            for idx in self.feat_idx:\n                feat_dim += feat_dims[idx]           \n            self.regressor_config.feat_dim = feat_dim            \n            self.ptype = ['center', 'center']\n            self.psize = config.regressor_config.psize            \n            self.pshift = config.regressor_config.pshift\n            self.panc = config.regressor_config.panc\n            self.shared = config.regressor_config.shared\n            self.regress_mid = FeatRegressNet(self.regressor_config, psize=self.psize[0])\n            if self.shared:\n                self.regress_fine = self.regress_mid\n                self.psize[1] = self.psize[0]\n            else:\n                self.regress_fine = FeatRegressNet(self.regressor_config, psize=self.psize[1])\n            \n        self.to(self.device)\n        self.init_weights_(weights_dict=config.weights_dict, pretrained=True)        \n           \n        if config.training:\n            self.freeze_feat = config.freeze_feat\n            # Freeze (part of) the backbone\n            print('Freezing feature extractor params upto layer {}'.format(self.freeze_feat))\n            for i, param in enumerate(self.extract.parameters()):         \n                # Resnet34 layer3=[48:87] blocks:\n                # 0=[48:57] 1=[57:63] 2=[63:69]\n                # 3=[69:75] 4=[75:81] 5=[81:87] \n                if i < self.freeze_feat:\n                    param.requires_grad = False\n\n                # Always freeze resnet layer4, since never used\n                if i >= 87:\n                    param.requires_grad = False        \n            \n            config.optim_config.start_epoch = config.start_epoch\n            self.set_optimizer_(config.optim_config)\n    \n    def set_optimizer_(self, optim_config): \n        params = []\n        if self.regress_mid:\n            params += list(self.regress_mid.parameters()) \n        if self.regress_fine and not self.shared:\n            params += list(self.regress_fine.parameters())         \n        params += list(self.ncn.parameters())\n        if self.freeze_feat < 87:\n            params += list(self.extract.parameters())[self.freeze_feat:87]\n        self.optimizer, self.lr_scheduler = init_optimizer(params, optim_config)\n        print('Init optimizer, items: {}'.format(len(params)))\n\n    def optim_step_(self, loss):\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()            \n                \n    def init_weights_(self, weights_dict=None, pretrained=True):\n        print('Xavier initialize all model parameters')\n        self.apply(xavier_init_func_)\n        if pretrained:\n            self.extract.load_pretrained_()\n        if weights_dict:\n            if len(weights_dict.items()) == len(self.state_dict()):\n                print('Reload all model parameters from weights dict')\n                self.load_state_dict(weights_dict)\n            else:\n                print('Reload part of model parameters from weights dict')\n                self.load_state_dict(weights_dict, strict=False)       \n                \n    def load_batch_(self, batch, dtype='pair'):\n        im_src = batch['src_im'].to(self.device)\n        im_pos = batch['pos_im'].to(self.device)\n        Fs = batch['F'].to(self.device)            \n        if dtype == 'triplet':\n            im_neg = batch['neg_im'].to(self.device)\n            return im_src, im_pos, im_neg, Fs\n        return im_src, im_pos, Fs    \n\n    def forward_coarse_match(self, feat1, feat2, ksize=1):\n        # Feature normalization\n        feat1 = L2Normalize(feat1, dim=1)\n        feat2 = L2Normalize(feat2, dim=1)\n        \n        # Feature correlation\n        corr4d = self.combine(feat1, feat2)\n        \n        # Do 4d maxpooling for relocalization\n        delta4d = None\n        if ksize > 1:\n            corr4d, max_i, max_j, max_k, max_l = maxpool4d(corr4d, k_size=ksize)\n            delta4d = (max_i,max_j,max_k,max_l)\n        corr4d = MutualMatching(corr4d)            \n        corr4d = self.ncn(corr4d)\n        corr4d = MutualMatching(corr4d)        \n        return corr4d, delta4d\n\n    def parse_regressor_out(self, out, psize, ptype, imatches, max_val):\n        w1, h1, w2, h2 = max_val\n        offset = out[:, :4] # N, 4\n        offset = psize * torch.tanh(nn.functional.relu(offset))        \n        if ptype == 'center':\n            shift = psize // 2\n            offset -= shift\n        fmatches = imatches.float() + offset\n        io_probs = out[:, 4]\n        io_probs = torch.sigmoid(io_probs)\n        \n        # Prevent out of range\n        x1 = fmatches[:, 0].clamp(min=0, max=w1)\n        y1 = fmatches[:, 1].clamp(min=0, max=h1)\n        x2 = fmatches[:, 2].clamp(min=0, max=w2)\n        y2 = fmatches[:, 3].clamp(min=0, max=h2)\n        fmatches = torch.stack([x1, y1, x2, y2], dim=-1)\n        return fmatches, io_probs\n\n    def forward_fine_match_mini_batch(self, feats1, feats2, ibatch, imatches,\n                                      psize, ptype, regressor):\n        # ibatch: to index the feature map\n        # imatches: input coarse matches, N, 4\n        N = imatches.shape[0]\n        _, _, h1, w1 = feats1[0].shape\n        _, _, h2, w2 = feats2[0].shape\n        max_val = [w1, h1, w2, h2]\n        \n        f1s, f2s, _, _ = select_local_patch_feats(feats1, feats2,\n                                                  ibatch, imatches,\n                                                  feat_idx=self.feat_idx,\n                                                  feats_downsample=self.feats_downsample,\n                                                  psize=psize,\n                                                  ptype=ptype)    \n        # Feature normalization\n        f1s = L2Normalize(f1s, dim=0)  # D, N*psize*psize\n        f2s = L2Normalize(f2s, dim=0)  # D, N*psize*psize\n\n        # Reshaping: -> (D, N, psize, psize) -> (N, D, psize, psize)\n        f1s = f1s.view(-1, N, psize, psize).permute(1, 0, 2, 3)\n        f2s = f2s.view(-1, N, psize, psize).permute(1, 0, 2, 3)   \n\n     \n        # From im1 to im2       \n        out = regressor(f1s, f2s)  # N, 5\n        fmatches, io_probs = self.parse_regressor_out(out, psize, ptype, imatches, max_val)\n        return fmatches, io_probs\n    \n    def forward_fine_match(self, feats1, feats2, coarse_matches, \n                           psize, ptype, regressor):\n        batch_size = self.regr_batch\n        masks = []\n        fine_matches = []\n        for ibatch, imatches in enumerate(coarse_matches):\n            # Use mini-batch if too many matches\n            N = imatches.shape[0]\n            if N > batch_size:                \n                batch_inds = [batch_size*i for i in range(N // batch_size + 1)]\n                if batch_inds[-1] < N:\n                    if N -  batch_inds[-1] == 1:\n                        # Special case, slicing leads to 1-dim missing\n                        batch_inds[-1] = N\n                    else:\n                        batch_inds += [N]\n                fmatches = []\n                io_probs = []\n                for bi, (ist, ied) in enumerate(zip(batch_inds[0:-1], batch_inds[1::])):\n                    mini_results = self.forward_fine_match_mini_batch(feats1, feats2, \n                                                                      ibatch, imatches[ist:ied],\n                                                                      psize, ptype, regressor)\n                    fmatches.append(mini_results[0])\n                    io_probs.append(mini_results[1])\n                fmatches = torch.cat(fmatches, dim=0).squeeze()\n                io_probs = torch.cat(io_probs, dim=0).squeeze()\n            else:\n                fmatches, io_probs = self.forward_fine_match_mini_batch(feats1, feats2, \n                                                                        ibatch, imatches,\n                                                                        psize, ptype, regressor)\n            fine_matches.append(fmatches)\n            masks.append(io_probs)                        \n        return fine_matches, masks\n\n    def forward(self, im1, im2, ksize=1, return_feats=False):\n        if return_feats:\n            feat1s=[]\n            feat2s=[]\n            self.extract.forward_all(im1, feat1s, early_feat=True)        \n            self.extract.forward_all(im2, feat2s, early_feat=True)\n            feat1 = feat1s[-1]\n            feat2 = feat2s[-1]\n        else:\n            feat1 = self.extract(im1, early_feat=True)        \n            feat2 = self.extract(im2, early_feat=True)     # Shared weights\n        \n        corr4d, delta4d = self.forward_coarse_match(feat1, feat2, ksize=ksize)\n        \n        if return_feats:\n            return corr4d, delta4d, feat1s, feat2s                            \n        else:\n            return corr4d, delta4d\n        \n        \n    def predict_coarse(self, im1, im2, ksize=2, ncn_thres=0.0,\n                       mutual=False, center=True):\n        corr4d, delta4d = self.forward(im1, im2, ksize)\n        coarse_matches, match_scores = self.cal_coarse_matches(corr4d, delta4d, ksize=ksize, \n                                                               upsample=self.upsample, center=center)\n        \n        # Filter coarse matches\n        coarse_matches, match_scores = filter_coarse(coarse_matches, match_scores, ncn_thres, mutual)\n        return coarse_matches, match_scores\n    \n    def predict_fine(self, im1, im2, ksize=2, ncn_thres=0.0,\n                     mutual=True, return_all=False):\n        corr4d, delta4d, feats1, feats2 = self.forward(im1, im2, ksize=ksize, return_feats=True)\n        coarse_matches, match_scores = self.cal_coarse_matches(corr4d, delta4d, ksize=ksize,\n                                                               upsample=self.upsample, center=True)\n        # Filter coarse matches\n        coarse_matches, match_scores = filter_coarse(coarse_matches, match_scores, ncn_thres, mutual)\n        \n        # Locate initial anchors\n        coarse_matches = self.shift_to_anchors(coarse_matches)\n        \n        # Mid level matching\n        mid_matches, mid_scores = self.forward_fine_match(feats1, feats2, \n                                                          coarse_matches, \n                                                          psize=self.psize[0],\n                                                          ptype=self.ptype[0],\n                                                          regressor=self.regress_mid)\n        \n        # Fine level matching\n        fine_matches, fine_scores = self.forward_fine_match(feats1, feats2, \n                                                            mid_matches,\n                                                            psize=self.psize[1],\n                                                            ptype=self.ptype[1],\n                                                            regressor=self.regress_fine)\n        if return_all:\n            return fine_matches, fine_scores, mid_matches, mid_scores, coarse_matches \n        return fine_matches, fine_scores, coarse_matches  \n    \n    def refine_matches(self, im1, im2, coarse_matches, io_thres):\n        # Handle empty coarse matches\n        if len(coarse_matches) == 0:\n            return np.empty((0, 4)), np.empty((0,)), np.empty((0, 4))\n\n        if type(coarse_matches) == np.ndarray:\n            coarse_matches_ = torch.from_numpy(coarse_matches).to(self.device).unsqueeze(0)  # 1, N, 4            \n        elif type(coarse_matches) == torch.Tensor:\n            coarse_matches_ = coarse_matches.unsqueeze(0)  # 1, N, 4\n            coarse_matches = coarse_matches.cpu().data.numpy()\n        \n        # Extract local features\n        feat1s=[]\n        feat2s=[]\n        self.extract.forward_all(im1, feat1s, early_feat=True)        \n        self.extract.forward_all(im2, feat2s, early_feat=True)    \n\n        # Mid level matching\n        mid_matches, mid_scores = self.forward_fine_match(feat1s, feat2s, \n                                                          coarse_matches_, \n                                                          psize=self.psize[0],\n                                                          ptype=self.ptype[0],\n                                                          regressor=self.regress_mid)\n\n        # Fine level matching\n        fine_matches, fine_scores = self.forward_fine_match(feat1s, feat2s, \n                                                            mid_matches,\n                                                            psize=self.psize[1],\n                                                            ptype=self.ptype[1],\n                                                            regressor=self.regress_fine)\n        refined_matches = fine_matches[0].cpu().data.numpy()\n        scores = fine_scores[0].cpu().data.numpy()\n        \n        # Further filtering with threshold\n        if io_thres > 0:\n            pos_ids = np.where(scores > io_thres)[0]\n            if len(pos_ids) > 0:\n                coarse_matches = coarse_matches[pos_ids]\n                refined_matches = refined_matches[pos_ids]\n                scores = scores[pos_ids]\n        return refined_matches, scores, coarse_matches\n    \n    def cal_coarse_score(self, corr4d, normalize='softmax'):\n        if normalize is None:\n            normalize = lambda x: x\n        elif normalize == 'softmax':     \n            normalize = lambda x: nn.functional.softmax(x, 1)\n        elif normalize == 'l1':\n            normalize = lambda x: x / (torch.sum(x, dim=1, keepdim=True) + 0.0001)\n        \n        # Mutual matching score\n        batch_size, _, h1, w1, h2, w2 = corr4d.shape\n        nc_B_Avec=corr4d.view(batch_size, h1*w1, h2, w2)\n        nc_A_Bvec=corr4d.view(batch_size, h1, w1, h2*w2).permute(0,3,1,2) # \n        nc_B_Avec = normalize(nc_B_Avec)\n        nc_A_Bvec = normalize(nc_A_Bvec)\n        scores_B,_= torch.max(nc_B_Avec, dim=1)\n        scores_A,_= torch.max(nc_A_Bvec, dim=1)\n        scores_AB = torch.cat([scores_A.view(-1, h1*w1), scores_B.view(-1, h2*w2)], dim=1)\n        score = scores_AB.mean()\n        return score\n    \n    def cal_coarse_matches(self, corr4d, delta4d, ksize=1, do_softmax=True,\n                           upsample=16, sort=False, center=True, pshift=0):\n        \n        # Original nc implementation: only max locations\n        (xA_, yA_, xB_, yB_, score_) = corr_to_matches(corr4d, delta4d=delta4d,\n                                                       do_softmax=do_softmax,\n                                                       ksize=ksize)\n        (xA2_, yA2_, xB2_, yB2_, score2_) = corr_to_matches(corr4d, delta4d=delta4d,\n                                                            do_softmax=do_softmax,\n                                                            ksize=ksize,\n                                                            invert_matching_direction=True)\n        xA_ = torch.cat((xA_, xA2_), 1)\n        yA_ = torch.cat((yA_, yA2_), 1)\n        xB_ = torch.cat((xB_, xB2_), 1)\n        yB_ = torch.cat((yB_, yB2_), 1)\n        score_ = torch.cat((score_, score2_),1)\n        \n        # Sort as descend\n        if sort:\n            sorted_index = torch.sort(-score_)[1]\n            xA_ = torch.gather(xA_, 1, sorted_index)  # B, 1, N\n            yA_ = torch.gather(yA_, 1, sorted_index)\n            xB_ = torch.gather(xB_, 1, sorted_index)\n            yB_ = torch.gather(yB_, 1, sorted_index)\n            score_ = torch.gather(score_, 1, sorted_index)  # B, N\n\n        xA_ = xA_.unsqueeze(1)\n        yA_ = yA_.unsqueeze(1)\n        xB_ = xB_.unsqueeze(1)\n        yB_ = yB_.unsqueeze(1)        \n        # Create matches and upscale to input resolution\n        matches_ = upsample * torch.cat([xA_, yA_, xB_, yB_], dim=1).permute(0, 2, 1) # B, N, 4\n        if center:\n            delta = upsample // 2\n            matches_ += torch.tensor([[delta, delta, delta, delta]]).unsqueeze(0).to(matches_)                        \n        return matches_, score_\n    \n    def shift_to_anchors(self, matches): \n        pshift = self.pshift\n        panc = self.panc\n        if panc == 1:\n            return matches\n        \n        # Move pt1/pt2 to its upper-left, upper-right, down-left, down-right\n        # location by pshift, leading to 4 corner anchors\n        # Then take center vs corner from two directions as new matches \n        shift_template = torch.tensor([\n            [-pshift, -pshift, 0, 0],\n            [pshift, -pshift,  0, 0],\n            [-pshift, pshift,  0, 0],\n            [pshift, pshift,   0, 0],\n            [0, 0, -pshift, -pshift],                \n            [0, 0, pshift, -pshift],\n            [0, 0, -pshift, pshift],\n            [0, 0, pshift, pshift]\n        ]).to(self.device)\n\n        matches_ = []\n        for imatches in matches:\n            imatches =  imatches.unsqueeze(1) + shift_template # N, 16, 4\n            imatches = imatches.reshape(-1, 4)\n            matches_.append(imatches)\n        return matches_\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T17:33:24.115429Z","iopub.execute_input":"2025-10-16T17:33:24.115710Z","iopub.status.idle":"2025-10-16T17:33:24.166377Z","shell.execute_reply.started":"2025-10-16T17:33:24.115689Z","shell.execute_reply":"2025-10-16T17:33:24.165346Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"import math\nimport torch\nfrom torch.nn.parameter import Parameter\nimport torch.nn.functional as F\nfrom torch.nn import Module\nfrom torch.nn.modules.conv import _ConvNd\nfrom torch.nn.modules.utils import _quadruple\nfrom torch.autograd import Variable\nfrom torch.nn import Conv2d\n\n\ndef conv4d(data, filters, bias=None, permute_filters=True, use_half=False):\n    b, c, h, w, d, t = data.size()\n\n    data = data.permute(\n        2, 0, 1, 3, 4, 5\n    ).contiguous()  # permute to avoid making contiguous inside loop\n\n    # Same permutation is done with filters, unless already provided with permutation\n    if permute_filters:\n        filters = filters.permute(\n            2, 0, 1, 3, 4, 5\n        ).contiguous()  # permute to avoid making contiguous inside loop\n\n    c_out = filters.size(1)\n    if use_half:\n        output = Variable(\n            torch.HalfTensor(h, b, c_out, w, d, t), requires_grad=data.requires_grad\n        )\n    else:\n        output = Variable(\n            torch.zeros(h, b, c_out, w, d, t), requires_grad=data.requires_grad\n        )\n\n    padding = filters.size(0) // 2\n    if use_half:\n        Z = Variable(torch.zeros(padding, b, c, w, d, t).half())\n    else:\n        Z = Variable(torch.zeros(padding, b, c, w, d, t))\n\n    if data.is_cuda:\n        Z = Z.cuda(data.get_device())\n        output = output.cuda(data.get_device())\n\n    data_padded = torch.cat((Z, data, Z), 0)\n\n    for i in range(output.size(0)):  # loop on first feature dimension\n        # convolve with center channel of filter (at position=padding)\n        output[i, :, :, :, :, :] = F.conv3d(\n            data_padded[i + padding, :, :, :, :, :],\n            filters[padding, :, :, :, :, :],\n            bias=bias,\n            stride=1,\n            padding=padding,\n        )\n        # convolve with upper/lower channels of filter (at postions [:padding] [padding+1:])\n        for p in range(1, padding + 1):\n            output[i, :, :, :, :, :] = output[i, :, :, :, :, :] + F.conv3d(\n                data_padded[i + padding - p, :, :, :, :, :],\n                filters[padding - p, :, :, :, :, :],\n                bias=None,\n                stride=1,\n                padding=padding,\n            )\n            output[i, :, :, :, :, :] = output[i, :, :, :, :, :] + F.conv3d(\n                data_padded[i + padding + p, :, :, :, :, :],\n                filters[padding + p, :, :, :, :, :],\n                bias=None,\n                stride=1,\n                padding=padding,\n            )\n\n    output = output.permute(1, 2, 0, 3, 4, 5).contiguous()\n    return output\n\n\nclass Conv4d(_ConvNd):\n    \"\"\"Applies a 4D convolution over an input signal composed of several input\n    planes.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        bias=True,\n        pre_permuted_filters=True,\n    ):\n        # stride, dilation and groups !=1 functionality not tested\n        stride = 1\n        dilation = 1\n        groups = 1\n        # zero padding is added automatically in conv4d function to preserve tensor size\n        padding = 0\n        kernel_size = _quadruple(kernel_size)\n        stride = _quadruple(stride)\n        padding = _quadruple(padding)\n        dilation = _quadruple(dilation)\n\n        super().__init__(\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride,\n            padding,\n            dilation,\n            False,\n            _quadruple(0),\n            groups,\n            bias,\n            padding_mode=\"zeros\",\n        )\n\n        # weights will be sliced along one dimension during convolution loop\n        # make the looping dimension to be the first one in the tensor,\n        # so that we don't need to call contiguous() inside the loop\n        self.pre_permuted_filters = pre_permuted_filters\n        if self.pre_permuted_filters:\n            self.weight.data = self.weight.data.permute(2, 0, 1, 3, 4, 5).contiguous()\n        self.use_half = False\n\n    def forward(self, input):\n        return conv4d(\n            input,\n            self.weight,\n            bias=self.bias,\n            permute_filters=not self.pre_permuted_filters,\n            use_half=self.use_half,\n        )  # filters pre-permuted in constructor\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T17:33:24.167912Z","iopub.execute_input":"2025-10-16T17:33:24.168232Z","iopub.status.idle":"2025-10-16T17:33:24.190272Z","shell.execute_reply.started":"2025-10-16T17:33:24.168211Z","shell.execute_reply":"2025-10-16T17:33:24.189244Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\ndef cal_conv_out_size(w, kernel_size, stride, padding):\n    \"\"\"Compute output spatial dimension after a conv layer.\"\"\"\n    return (w + 2 * padding - (kernel_size - 1) - 1) // stride + 1\n\nclass FeatRegressNet(nn.Module):\n    \"\"\"\n    Feature Regressor Network\n    -------------------------\n    - Takes a pair of local patches (feat1, feat2)\n    - Combines them (either pre- or post-conv)\n    - Regresses a 5-D vector [dx1, dy1, dx2, dy2, io_prob]\n    \"\"\"\n\n    def __init__(self, config, psize=16, out_dim=5):\n        super().__init__()\n        self.psize = psize\n\n        # --- Required configs ---\n        self.conv_kers = config['conv_kers']\n        self.conv_dims = config['conv_dims']\n        self.conv_strs = config['conv_strs'] if 'conv_strs' in config else [2] * len(config['conv_kers'])\n        self.feat_comb = config['feat_comb']\n        self.feat_dim = config['feat_dim']\n\n        # feature combiner affects input/output dims\n        if self.feat_comb == 'post':\n            self.fc_in_dim = config['conv_dims'][-1] * 2\n        else:\n            self.fc_in_dim = config['conv_dims'][-1]\n\n        self.fc_dims = config['fc_dims']\n\n        # --- Build layers ---\n        in_channels = self.feat_dim if self.feat_comb == 'post' else self.feat_dim * 2\n        self.conv = self.make_conv_layers(in_channels, self.conv_dims, self.conv_kers)\n        self.fc = self.make_fc_layers(self.fc_in_dim, self.fc_dims, out_dim)\n\n        print(f\"FeatRegressNet initialized -> \"\n              f\"feat_comb: {self.feat_comb}, psize: {self.psize}, \"\n              f\"conv_dims: {self.conv_dims}, conv_kers: {self.conv_kers}, \"\n              f\"conv_strs: {self.conv_strs}, fc_dims: {self.fc_dims}\")\n\n    def make_conv_layers(self, in_dim, conv_dims, conv_kers, bias=False):\n        layers = []\n        w = self.psize  # initial spatial size\n        for out_dim, kernel_size, stride in zip(conv_dims, conv_kers, self.conv_strs):\n            layers += [\n                nn.Conv2d(in_dim, out_dim, kernel_size, stride=stride, padding=1, bias=bias),\n                nn.BatchNorm2d(out_dim),\n                nn.ReLU(inplace=True)\n            ]\n            w = cal_conv_out_size(w, kernel_size, stride, 1)\n            in_dim = out_dim\n        # reduce to 1x1 spatial map\n        layers.append(nn.MaxPool2d(kernel_size=w))\n        return nn.Sequential(*layers)\n\n    def make_fc_layers(self, in_dim, fc_dims, fc_out_dim):\n        layers = []\n        for out_dim in fc_dims:\n            layers += [nn.Linear(in_dim, out_dim), nn.BatchNorm1d(out_dim), nn.ReLU(inplace=True)]\n            in_dim = out_dim\n        layers.append(nn.Linear(in_dim, fc_out_dim))\n        return nn.Sequential(*layers)\n\n    def forward(self, feat1, feat2):\n        # feat1, feat2: shape (N, D, psize, psize)\n        if self.feat_comb == 'pre':\n            feat = torch.cat([feat1, feat2], dim=1)\n            feat = self.conv(feat)\n        else:\n            feat1 = self.conv(feat1)\n            feat2 = self.conv(feat2)\n            feat = torch.cat([feat1, feat2], dim=1)\n        feat = feat.view(feat.size(0), -1)\n        out = self.fc(feat)\n        return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T17:33:24.191310Z","iopub.execute_input":"2025-10-16T17:33:24.191704Z","iopub.status.idle":"2025-10-16T17:33:24.211763Z","shell.execute_reply.started":"2025-10-16T17:33:24.191670Z","shell.execute_reply":"2025-10-16T17:33:24.210763Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\ndef cal_conv_out_size(w, kernel_size, stride, padding):\n    \"\"\"Compute output spatial dimension after a conv layer.\"\"\"\n    return (w + 2 * padding - (kernel_size - 1) - 1) // stride + 1\n\nclass FeatRegressNet(nn.Module):\n    def __init__(self, config, psize=16, out_dim=5):\n        super().__init__()\n        self.psize = psize\n\n        # ✅ Use attribute access (not dict style)\n        self.conv_strs = config.conv_strs if hasattr(config, 'conv_strs') else [2] * len(config.conv_kers)\n        self.conv_dims = config.conv_dims\n        self.conv_kers = config.conv_kers\n        self.feat_comb = config.feat_comb\n\n        # Determine feature dims\n        self.feat_dim = config.feat_dim if self.feat_comb == 'post' else 2 * config.feat_dim\n        self.fc_in_dim = config.conv_dims[-1] * 2 if self.feat_comb == 'post' else config.conv_dims[-1]\n        self.fc_dims = config.fc_dims\n\n        # Build conv and FC stacks\n        self.conv = self.make_conv_layers(self.feat_dim, self.conv_dims, self.conv_kers)\n        self.fc = self.make_fc_layers(self.fc_in_dim, self.fc_dims, out_dim)\n\n        print(f'FeatRegressNet: feat_comb:{self.feat_comb} '\n              f'psize:{self.psize} out:{out_dim} '\n              f'feat_dim:{self.feat_dim} conv_kers:{self.conv_kers} '\n              f'conv_dims:{self.conv_dims} conv_str:{self.conv_strs}')\n\n    def make_conv_layers(self, in_dim, conv_dims, conv_kers, bias=False):\n        layers = []\n        w = self.psize\n        for out_dim, kernel_size, stride in zip(conv_dims, conv_kers, self.conv_strs):\n            layers.append(nn.Conv2d(in_dim, out_dim, kernel_size, stride=stride, padding=1, bias=bias))\n            layers.append(nn.BatchNorm2d(out_dim))\n            layers.append(nn.ReLU(inplace=True))\n            w = cal_conv_out_size(w, kernel_size, stride, 1)\n            in_dim = out_dim\n        layers.append(nn.MaxPool2d(kernel_size=w))\n        return nn.Sequential(*layers)\n\n    def make_fc_layers(self, in_dim, fc_dims, fc_out_dim):\n        layers = []\n        for out_dim in fc_dims:\n            layers.append(nn.Linear(in_dim, out_dim))\n            layers.append(nn.BatchNorm1d(out_dim))\n            layers.append(nn.ReLU(inplace=True))\n            in_dim = out_dim\n        layers.append(nn.Linear(in_dim, fc_out_dim))\n        return nn.Sequential(*layers)\n\n    def forward(self, feat1, feat2):\n        # feat1, feat2: (N, D, psize, psize)\n        if self.feat_comb == 'pre':\n            feat = torch.cat([feat1, feat2], dim=1)\n            feat = self.conv(feat)\n        else:\n            feat1 = self.conv(feat1)\n            feat2 = self.conv(feat2)\n            feat = torch.cat([feat1, feat2], dim=1)\n        feat = feat.view(feat.size(0), -1)\n        out = self.fc(feat)\n        return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T17:33:24.213752Z","iopub.execute_input":"2025-10-16T17:33:24.214078Z","iopub.status.idle":"2025-10-16T17:33:24.236152Z","shell.execute_reply.started":"2025-10-16T17:33:24.214045Z","shell.execute_reply":"2025-10-16T17:33:24.234984Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"config.regressor_config.feat_dim = 518\n\nckpt = torch.load(\"patch2pix_pretrained.pth\", map_location=device)\nstate_dict = ckpt.get(\"state_dict\", ckpt)\nmissing, unexpected = model.load_state_dict(state_dict, strict=False)\nprint(\"✅ Loaded pretrained Patch2Pix weights\")\nprint(\"Missing:\", missing)\nprint(\"Unexpected:\", unexpected)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T17:46:55.589012Z","iopub.execute_input":"2025-10-16T17:46:55.589754Z","iopub.status.idle":"2025-10-16T17:46:55.767986Z","shell.execute_reply.started":"2025-10-16T17:46:55.589704Z","shell.execute_reply":"2025-10-16T17:46:55.766531Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/3949270476.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"patch2pix_pretrained.pth\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"state_dict\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"✅ Loaded pretrained Patch2Pix weights\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Missing:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2581\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2582\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2583\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Patch2Pix:\n\tsize mismatch for regress_mid.conv.0.weight: copying a param with shape torch.Size([512, 518, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 448, 3, 3]).\n\tsize mismatch for regress_mid.conv.3.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for regress_mid.fc.0.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([512, 1024]).\n\tsize mismatch for regress_fine.conv.0.weight: copying a param with shape torch.Size([512, 518, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 448, 3, 3]).\n\tsize mismatch for regress_fine.conv.3.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for regress_fine.fc.0.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([512, 1024])."],"ename":"RuntimeError","evalue":"Error(s) in loading state_dict for Patch2Pix:\n\tsize mismatch for regress_mid.conv.0.weight: copying a param with shape torch.Size([512, 518, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 448, 3, 3]).\n\tsize mismatch for regress_mid.conv.3.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for regress_mid.fc.0.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([512, 1024]).\n\tsize mismatch for regress_fine.conv.0.weight: copying a param with shape torch.Size([512, 518, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 448, 3, 3]).\n\tsize mismatch for regress_fine.conv.3.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for regress_fine.fc.0.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([512, 1024]).","output_type":"error"}],"execution_count":61},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom types import SimpleNamespace\n\n\n# --- Load configuration ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Dummy regressor config (not training)\nRegressorCfg = SimpleNamespace(\n    feat_dim=None, psize=[7, 5], pshift=2, panc=1,\n    shared=True, freeze=False\n)\n\n\nfrom types import SimpleNamespace\n\nRegressorCfg = SimpleNamespace(\n    feat_dim=None,                # filled automatically\n    conv_kers=[3, 3, 3],\n    conv_dims=[512, 512, 512],    # <- official values\n    conv_strs=[2, 2, 1],\n    feat_comb='post',             # <- very important\n    fc_dims=[512, 256],           # <- official values\n    psize=[7, 5],\n    pshift=2,\n    panc=1,\n    shared=True,\n    freeze=False\n)\n\nconfig = SimpleNamespace(\n    device=device,\n    backbone='ResNet34',\n    change_stride=True,\n    regressor_config=RegressorCfg,\n    regr_batch=128,\n    feat_idx=[2, 3, 4],\n    weights_dict=None,\n    training=False\n)\n\n\n# --- Initialize model ---\nmodel = Patch2Pix(config)\nprint(\"✅ Patch2Pix initialized\")\n\n# --- Load pretrained weights ---\nckpt = torch.load(\"patch2pix_pretrained.pth\", map_location=device)\nstate_dict = ckpt.get(\"state_dict\", ckpt)\nmissing, unexpected = model.load_state_dict(state_dict, strict=False)\nprint(f\"✅ Loaded pretrained Patch2Pix weights\\nMissing: {missing}\\nUnexpected: {unexpected}\")\nmodel.eval()\n\n# --- Load and preprocess two example images ---\ndef load_image(path, size=256):\n    img = cv2.imread(path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, (size, size))\n    tensor = torch.from_numpy(img).permute(2,0,1).float().unsqueeze(0) / 255.\n    return tensor.to(device), img\n\n# Example: use any two similar images\nurl1 = \"https://upload.wikimedia.org/wikipedia/commons/9/9a/Taj_Mahal_in_March_2004.jpg\"\nurl2 = \"https://upload.wikimedia.org/wikipedia/commons/d/da/Taj_Mahal_from_garden_1.jpg\"\n!wget -q {url1} -O img1.jpg\n!wget -q {url2} -O img2.jpg\n\nim1_t, im1 = load_image(\"img1.jpg\")\nim2_t, im2 = load_image(\"img2.jpg\")\n\n# --- Run Patch2Pix inference ---\nwith torch.no_grad():\n    fine_matches, fine_scores, coarse_matches = model.predict_fine(im1_t, im2_t, ksize=2)\n\nmatches = fine_matches[0].cpu().numpy()\nscores = fine_scores[0].cpu().numpy()\nprint(f\"✅ Got {len(matches)} matches\")\n\n# --- Visualization ---\ndef draw_matches(im1, im2, matches, scores, N=50):\n    # Sort by score\n    idx = np.argsort(-scores)[:N]\n    matches = matches[idx].astype(int)\n    h1, w1, _ = im1.shape\n    h2, w2, _ = im2.shape\n    canvas = np.zeros((max(h1,h2), w1+w2, 3), dtype=np.uint8)\n    canvas[:h1, :w1] = im1\n    canvas[:h2, w1:] = im2\n    for (x1,y1,x2,y2) in matches:\n        color = tuple(np.random.randint(0,255,3).tolist())\n        cv2.line(canvas, (x1,y1), (x2+w1,y2), color, 1)\n        cv2.circle(canvas, (x1,y1), 3, color, -1)\n        cv2.circle(canvas, (x2+w1,y2), 3, color, -1)\n    return canvas\n\ncanvas = draw_matches(im1, im2, matches, scores, N=100)\nplt.figure(figsize=(12,6))\nplt.imshow(canvas)\nplt.axis('off')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T17:44:35.920936Z","iopub.execute_input":"2025-10-16T17:44:35.921272Z","iopub.status.idle":"2025-10-16T17:44:36.778192Z","shell.execute_reply.started":"2025-10-16T17:44:35.921248Z","shell.execute_reply":"2025-10-16T17:44:36.777078Z"}},"outputs":[{"name":"stdout","text":"Initialize Patch2Pix: backbone=ResNet34 cstride=True upsample=8\nInit regressor namespace(feat_dim=None, conv_kers=[3, 3, 3], conv_dims=[512, 512, 512], conv_strs=[2, 2, 1], feat_comb='post', fc_dims=[512, 256], psize=[7, 5], pshift=2, panc=1, shared=True, freeze=False)\nFeatRegressNet: feat_comb:post psize:7 out:5 feat_dim:448 conv_kers:[3, 3, 3] conv_dims:[512, 512, 512] conv_str:[2, 2, 1]\nXavier initialize all model parameters\nInitialize ResNet using pretrained model from https://download.pytorch.org/models/resnet34-333f7ec4.pth\n✅ Patch2Pix initialized\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/2088599635.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"patch2pix_pretrained.pth\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"state_dict\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"✅ Loaded pretrained Patch2Pix weights\\nMissing: {missing}\\nUnexpected: {unexpected}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2581\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2582\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2583\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Patch2Pix:\n\tsize mismatch for regress_mid.conv.0.weight: copying a param with shape torch.Size([512, 518, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 448, 3, 3]).\n\tsize mismatch for regress_mid.conv.3.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for regress_mid.fc.0.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([512, 1024]).\n\tsize mismatch for regress_fine.conv.0.weight: copying a param with shape torch.Size([512, 518, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 448, 3, 3]).\n\tsize mismatch for regress_fine.conv.3.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for regress_fine.fc.0.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([512, 1024])."],"ename":"RuntimeError","evalue":"Error(s) in loading state_dict for Patch2Pix:\n\tsize mismatch for regress_mid.conv.0.weight: copying a param with shape torch.Size([512, 518, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 448, 3, 3]).\n\tsize mismatch for regress_mid.conv.3.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for regress_mid.fc.0.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([512, 1024]).\n\tsize mismatch for regress_fine.conv.0.weight: copying a param with shape torch.Size([512, 518, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 448, 3, 3]).\n\tsize mismatch for regress_fine.conv.3.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for regress_fine.fc.0.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([512, 1024]).","output_type":"error"}],"execution_count":59},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}